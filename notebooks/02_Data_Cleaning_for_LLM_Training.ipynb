{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM 훈련 데이터 정제\n",
        "## 당혹도 계산"
      ],
      "metadata": {
        "id": "c5FEU2G8UhfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hz3QCEuzOi82"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    return torch.exp(outputs.loss).item()\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "clean_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "noisy_text = \"Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g.\"\n",
        "\n",
        "clean_perplexity = calculate_perplexity(model, tokenizer, clean_text)\n",
        "noisy_perplexity = calculate_perplexity(model, tokenizer, noisy_text)\n",
        "\n",
        "print(\"깨끗한 텍스트 당혹도:\", clean_perplexity)\n",
        "print(\"노이즈 텍스트 당혹도:\", noisy_perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGzOVj1oOqcb",
        "outputId": "e59eb4ac-9e4f-47b5-ff17-d8f6cbb18ccf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "깨끗한 텍스트 당혹도: 162.47128295898438\n",
            "노이즈 텍스트 당혹도: 587.935302734375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 품질 검사"
      ],
      "metadata": {
        "id": "ITt3N_oVUpCt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fb64264",
        "outputId": "274db6c9-8546-49a3-e033-ec915b451c0a"
      },
      "source": [
        "%pip install pyspellchecker"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/7.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spell = SpellChecker()\n",
        "\n",
        "def analyze_text_quality(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 철자 검사\n",
        "    misspelled = [token.text for token in doc if token.is_alpha and token.text.lower() in spell.unknown([token.text.lower()])]\n",
        "\n",
        "    # 문법 점수 (간단한 품사 카운트)\n",
        "    pos_counts = Counter(token.pos_ for token in doc)\n",
        "    grammar_score = pos_counts['NOUN'] + pos_counts['VERB'] + pos_counts['ADJ'] + pos_counts['ADV']\n",
        "\n",
        "    # 문장 완전성 검사\n",
        "    incomplete_sentences = [sent.text for sent in doc.sents if len(sent) < 3]\n",
        "\n",
        "    return {\n",
        "        \"misspelled_words\": misspelled,\n",
        "        \"grammar_score\": grammar_score,\n",
        "        \"incomplete_sentences\": incomplete_sentences\n",
        "    }\n",
        "\n",
        "text = \"This iz a smple txt with sum issues. Incomplet\"\n",
        "quality_report = analyze_text_quality(text)\n",
        "print(quality_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH-QMUriWknT",
        "outputId": "4a878e83-89ea-4ef8-bb20-a0fcb78efe24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'misspelled_words': ['iz', 'smple', 'txt', 'Incomplet'], 'grammar_score': 5, 'incomplete_sentences': ['Incomplet']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 전처리"
      ],
      "metadata": {
        "id": "cWPRpdF4YJne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# 필요한 NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bn1Ej3zYK72",
        "outputId": "5dbcf857-6778-4b08-e2e0-b0f5b13a0e85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # 텍스트를 소문자화\n",
        "    text = text.lower()\n",
        "\n",
        "    # 유니코드 캐릭터 정규화\n",
        "    text = unicodedata.normalize(\n",
        "        'NFKD', text\n",
        "    ).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "    # 구두점 제거\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 화이트스페이스 정규화\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # 토큰화\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [\n",
        "        token for token in tokens if token not in stop_words\n",
        "    ]\n",
        "\n",
        "    # 토큰을 다시 텍스트로 결합\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "556YAo88YReB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"This is an EXAMPLE of text preprocessing... It's quite useful!\"\n",
        "cleaned_text = preprocess_text(raw_text)\n",
        "print(f\"원문: {raw_text}\")\n",
        "print(f\"전처리 결과: {cleaned_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8LJZ0dkYxo5",
        "outputId": "4912ac4c-abd9-4cf1-ec60-ef62370385d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: This is an EXAMPLE of text preprocessing... It's quite useful!\n",
            "전처리 결과: example text preprocessing quite useful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 언어 감지 및 정규화"
      ],
      "metadata": {
        "id": "GErsB71vZebV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a87016b",
        "outputId": "8187b552-5363-4036-cd64-50d25cd33368"
      },
      "source": [
        "%pip install langdetect"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m696.3/981.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=efc9f5e8611a29508256dd4bef7fd78d8d037bd32321011d1365b25e9746f25e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f953290c",
        "outputId": "40a60c80-3f74-4672-b0d0-30d563c385f4"
      },
      "source": [
        "%pip install unidecode"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "from unidecode import unidecode\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# 필요한 NLTK 데이터를 다운로드\n",
        "nltk.download('punkt')\n",
        "\n",
        "def handle_multilingual_text(text):\n",
        "    # 언어 감지\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = 'unknown'\n",
        "\n",
        "    # 비ASCII 캐릭터를 음역\n",
        "    transliterated_text = unidecode(text)\n",
        "\n",
        "    tokens = word_tokenize(transliterated_text)\n",
        "\n",
        "    return {\n",
        "        'original': text,\n",
        "        'language': lang,\n",
        "        'transliterated': transliterated_text,\n",
        "        'tokens': tokens\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1yZodqPZh2Z",
        "outputId": "57667e68-d6e6-42ca-e4f8-35192555d7e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"This is English text.\",\n",
        "    \"Dies ist deutscher Text.\",\n",
        "    \"これは日本語のテキストです。\",\n",
        "    \"This is mixed language text avec un peu de français.\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    result = handle_multilingual_text(text)\n",
        "    print(f\"원문: {result['original']}\")\n",
        "    print(f\"언어: {result['language']}\")\n",
        "    print(f\"번역문: {result['transliterated']}\")\n",
        "    print(f\"토큰: {result['tokens']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTnawCcwZsfI",
        "outputId": "e0524f85-ca72-4aec-b702-b33c4175d46b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: This is English text.\n",
            "언어: en\n",
            "번역문: This is English text.\n",
            "토큰: ['This', 'is', 'English', 'text', '.']\n",
            "\n",
            "원문: Dies ist deutscher Text.\n",
            "언어: de\n",
            "번역문: Dies ist deutscher Text.\n",
            "토큰: ['Dies', 'ist', 'deutscher', 'Text', '.']\n",
            "\n",
            "원문: これは日本語のテキストです。\n",
            "언어: ja\n",
            "번역문: korehaRi Ben Yu notekisutodesu. \n",
            "토큰: ['korehaRi', 'Ben', 'Yu', 'notekisutodesu', '.']\n",
            "\n",
            "원문: This is mixed language text avec un peu de français.\n",
            "언어: fr\n",
            "번역문: This is mixed language text avec un peu de francais.\n",
            "토큰: ['This', 'is', 'mixed', 'language', 'text', 'avec', 'un', 'peu', 'de', 'francais', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 중복 제거"
      ],
      "metadata": {
        "id": "I7SW0x6gaJGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def deduplicate_corpus(corpus, similarity_threshold=0.9):\n",
        "    # TF-IDF 벡터화기를 생성\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    # 쌍별 유사도 계산\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    duplicates = set()\n",
        "    for i in range(len(corpus)):\n",
        "        for j in range(i + 1, len(corpus)):\n",
        "            if similarity_matrix[i, j] > similarity_threshold:\n",
        "                duplicates.add(j)\n",
        "\n",
        "    deduplicated_corpus = [\n",
        "        doc for i, doc in enumerate(corpus)\n",
        "        if i not in duplicates\n",
        "    ]\n",
        "\n",
        "    return deduplicated_corpus\n"
      ],
      "metadata": {
        "id": "v94V70qnaMUY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast auburn fox leaps above the sleepy canine.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"An entirely different sentence about cats.\",\n",
        "]\n",
        "\n",
        "deduplicated = deduplicate_corpus(corpus)\n",
        "print(f\"원래 말뭉치 크기: {len(corpus)}\")\n",
        "print(f\"중복 제거된 말뭉치 크기: {len(deduplicated)}\")\n",
        "print(\"중복 제거된 말뭉치:\")\n",
        "for doc in deduplicated:\n",
        "    print(f\"- {doc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VSpHGZAaV9_",
        "outputId": "4c4f1e50-86af-4b52-f6f9-1b29172e709a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 말뭉치 크기: 4\n",
            "중복 제거된 말뭉치 크기: 3\n",
            "중복 제거된 말뭉치:\n",
            "- The quick brown fox jumps over the lazy dog.\n",
            "- A fast auburn fox leaps above the sleepy canine.\n",
            "- An entirely different sentence about cats.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 정제 파이프라인 자동화"
      ],
      "metadata": {
        "id": "Vyq3reqBacmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "# 필요한 NLTK 데이터 다운로드\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "class DataCleaningPipeline:\n",
        "    def __init__(\n",
        "        self, similarity_threshold=0.9, min_length=10,\n",
        "        max_length=1000\n",
        "    ):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        # 기본 전처리\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        tokens = [\n",
        "            word for word in text.split()\n",
        "            if word not in stop_words\n",
        "        ]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def filter_by_length(self, df):\n",
        "        return df[\n",
        "            (df['text'].str.len() >= self.min_length) &\n",
        "            (df['text'].str.len() <= self.max_length)\n",
        "        ]\n",
        "\n",
        "    def deduplicate(self, df):\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(df['text'])\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "        duplicates = set()\n",
        "        for i in range(len(df)):\n",
        "            for j in range(i + 1, len(df)):\n",
        "                if similarity_matrix[i, j] > self.similarity_threshold:\n",
        "                    duplicates.add(j)\n",
        "\n",
        "        return df.drop(df.index[list(duplicates)])\n",
        "\n",
        "    def clean(self, input_file, output_file):\n",
        "        # 데이터 읽기\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # 전처리\n",
        "        df['text'] = df['text'].apply(self.preprocess)\n",
        "\n",
        "        # 길이로 필터링\n",
        "        df = self.filter_by_length(df)\n",
        "\n",
        "        # 중복 제거\n",
        "        df = self.deduplicate(df)\n",
        "\n",
        "        # 정제한 데이터 저장\n",
        "        df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"정제한 데이터를 {output_file}에 저장했습니다.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdiotqSwagS2",
        "outputId": "f0517204-79a1-4172-930a-ed0c5b361071"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 간단한 예제 데이터프레임\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"This is a clean sentence for testing.\",\n",
        "        \"This is a clean sentence for testing.\",   # 중복\n",
        "        \"Short\",                                   # 너무 짧음\n",
        "        \"Another example sentence with some noise!!!\",\n",
        "        \"This text is way toooooooooooooooooooooooooooooooooooooooooooooooooo long\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# CSV로 저장\n",
        "pd.DataFrame(data).to_csv(\"input_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "tAdrAPjCcbtj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = DataCleaningPipeline()\n",
        "pipeline.clean('input_data.csv', 'cleaned_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HV31rWQcHZV",
        "outputId": "e424937f-0bde-430b-bcb7-3e27e33c5a81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정제한 데이터를 cleaned_data.csv에 저장했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.read_csv(\"cleaned_data.csv\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ0vb-w_chBB",
        "outputId": "7750f865-9ac1-4fb4-c0de-78d96801d2c3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0                             clean sentence testing\n",
            "1                     another example sentence noise\n",
            "2  text way toooooooooooooooooooooooooooooooooooo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 검증"
      ],
      "metadata": {
        "id": "s7USzmuAdPsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_cleaned_data(file_path, sample_size=100):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # 기본 통계\n",
        "    print(f\"전체 샘플: {len(df)}\")\n",
        "    print(\n",
        "        f\"평균 텍스트 길이: \"\n",
        "        f\"{df['text'].str.len().mean():.2f}\"\n",
        "    )\n",
        "\n",
        "    print(f\"고유 샘플: {df['text'].nunique()}\")\n",
        "\n",
        "    short_texts = df[df['text'].str.len() < 10]\n",
        "    print(\n",
        "        f\"10자 미만의 텍스트 수: \"\n",
        "        f\"{len(short_texts)}\"\n",
        "    )\n",
        "\n",
        "    sample = df.sample(n=min(sample_size, len(df)))\n",
        "    print(\"\\n수동 검토를 위한 샘플:\")\n",
        "    print(sample['text'].head())\n",
        "\n",
        "    # 일반적인 문제 확인\n",
        "    common_issues = {\n",
        "        'special_chars': df['text'].str.contains(\n",
        "            r'[^a-zA-Z0-9\\s]'\n",
        "        ),\n",
        "        'numbers': df['text'].str.contains(r'\\d'),\n",
        "        'all_caps': df['text'].str.isupper()\n",
        "    }\n",
        "    for issue, mask in common_issues.items():\n",
        "        print(f\"{issue}이 포함된 샘플: {mask.sum()}\")\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    def calculate_perplexity(text):\n",
        "        inputs = tokenizer(\n",
        "            text, return_tensors='pt', truncation=True, max_length=1024\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "        return torch.exp(outputs.loss).item()\n",
        "\n",
        "    sample_perplexities = sample['text'].apply(calculate_perplexity)\n",
        "    print(\n",
        "            f\"\\n샘플의 평균 당혹도: \"\n",
        "            f\"{sample_perplexities.mean():.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "QiDmlWf_ciKc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_cleaned_data('cleaned_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh-Aw-GHduyT",
        "outputId": "7e606956-99e0-4d46-ead2-1d6fb98ec7f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플: 3\n",
            "평균 텍스트 길이: 39.00\n",
            "고유 샘플: 3\n",
            "10자 미만의 텍스트 수: 0\n",
            "\n",
            "수동 검토를 위한 샘플:\n",
            "0                               clean sentence testing\n",
            "1                       another example sentence noise\n",
            "2    text way toooooooooooooooooooooooooooooooooooo...\n",
            "Name: text, dtype: object\n",
            "special_chars이 포함된 샘플: 0\n",
            "numbers이 포함된 샘플: 0\n",
            "all_caps이 포함된 샘플: 0\n",
            "\n",
            "샘플의 평균 당혹도: 49874.19\n"
          ]
        }
      ]
    }
  ]
}