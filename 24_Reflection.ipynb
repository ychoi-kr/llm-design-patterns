{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 자기반성을 위한 프롬프트 설계"
      ],
      "metadata": {
        "id": "Nsc4U5-fVXsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZVXRLd2VMqW"
      },
      "outputs": [],
      "source": [
        "def Reflection_prompt(task, initial_response):\n",
        "    prompt = f\"\"\"과업: {task}\n",
        "\n",
        "초기 응답:\n",
        "{initial_response}\n",
        "\n",
        "이제 자기반성을 해보자:\n",
        "\n",
        "1. 초기 응답의 장점과 단점을 평가한다.\n",
        "2. 오류, 비일관적 요소, 개선이 필요한 부분을 식별한다.\n",
        "3. 응답을 향상시키기 위한 구체적인 방법을 제안한다.\n",
        "4. 수정되고 개선된 응답 버전을 제공한다.\n",
        "\n",
        "자기반성과 개선된 응답:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# 사용 예\n",
        "task = \"고등학생에게 양자 얽힘의 개념을 설명하라.\"\n",
        "initial_response = \"양자 얽힘은 두 입자가 연결되어 있어, 하나를 측정하면 얼마나 멀리 떨어져 있든지 간에 다른 하나에 즉시 영향을 미치는 현상입니다.\"\n",
        "\n",
        "prompt = Reflection_prompt(task, initial_response)\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 반복적 개선 구현"
      ],
      "metadata": {
        "id": "rjEn5ckIVb1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def iterative_Reflection(model, tokenizer, task, max_iterations=3):\n",
        "    response = generate_initial_response(model, tokenizer, task)\n",
        "    for i in range(max_iterations):\n",
        "        prompt = Reflection_prompt(task, response)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = model.generate(\n",
        "            inputs, max_length=1000, num_return_sequences=1\n",
        "        )\n",
        "        reflection = tokenizer.decode(outputs[0],\n",
        "            skip_special_tokens=True)\n",
        "\n",
        "        # 반성에서 개선된 응답을 추출\n",
        "        response = extract_improved_response(reflection)\n",
        "        if is_satisfactory(response):\n",
        "            break\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "6_GkqCvAVe9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_response(model, tokenizer, task):\n",
        "    prompt = f\"과업: {task}\\n\\n응답:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=500, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def extract_improved_response(reflection):\n",
        "    # 반성에서 향상된 응답을 추출하는 로직을 구현한다\n",
        "    # 이는 텍스트 구문 분석이나 생성된 텍스트의 마커 사용을 포함할 수 있다\n",
        "    pass\n",
        "\n",
        "def is_satisfactory(response):\n",
        "    # 응답이 품질 기준을 충족하는지 판단하는 로직을 구현한다\n",
        "    # 이는 길이 검사, 키워드 존재 여부, 또는 더 고급 지표를 포함할 수 있다\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "eNSpXQDWVk8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2-large\"  # 선호하는 모델로 교체\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "task = \"식물에서 광합성 과정을 설명하라.\"\n",
        "final_response = iterative_Reflection(model, tokenizer, task)\n",
        "print(final_response)\n"
      ],
      "metadata": {
        "id": "dw611goxVqGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 오류 수정"
      ],
      "metadata": {
        "id": "maH5rEYPVrwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_correction_Reflection(model, tokenizer, task, initial_response, known_errors):\n",
        "    prompt = f\"\"\"과업: {task}\n",
        "\n",
        "초기 응답:\n",
        "{initial_response}\n",
        "\n",
        "알려진 오류:\n",
        "{' '.join(f'- {error}' for error in known_errors)}\n",
        "\n",
        "초기 응답을 반성해 알려진 오류를 수정하는 데 중점을 두어라. 이러한 문제를 해결한 개선된 응답을 제공해라.\n",
        "\n",
        "수정된 응답:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
        "    corrected_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_response\n",
        "\n",
        "# 사용 예\n",
        "task = \"원자의 구조를 설명해라.\"\n",
        "initial_response = \"원자는 양성자와 중성자를 포함하는 핵과, 그 주위를 고정된 원형 궤도로 도는 전자로 구성됩니다.\"\n",
        "known_errors = [\n",
        "    \"전자는 고정된 원형 경로로 돌지 않는다\",\n",
        "    \"설명에 전자 껍질이나 에너지 준위가 언급되지 않았다\"\n",
        "]\n",
        "\n",
        "corrected_response = error_correction_Reflection(\n",
        "    model, tokenizer, task, initial_response, known_errors\n",
        ")\n",
        "print(corrected_response)\n"
      ],
      "metadata": {
        "id": "XCrhQQR5VtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 반성의 영향 평가"
      ],
      "metadata": {
        "id": "7U_4epcDV8CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_Reflection_impact(initial_response, Reflection_response, criteria):\n",
        "    initial_scores = evaluate_response(initial_response, criteria)\n",
        "    Reflection_scores = evaluate_response(Reflection_response, criteria)\n",
        "    impact = {\n",
        "        criterion: Reflection_scores[criterion] - initial_scores[criterion]\n",
        "        for criterion in criteria\n",
        "    }\n",
        "    return {\n",
        "        \"initial_scores\": initial_scores,\n",
        "        \"Reflection_scores\": Reflection_scores,\n",
        "        \"impact\": impact\n",
        "    }\n",
        "\n",
        "def evaluate_response(response, criteria):\n",
        "    scores = {}\n",
        "    for criterion in criteria:\n",
        "        # 기준별 평가 논리를 구현\n",
        "        scores[criterion] = evaluate_criterion(response, criterion)\n",
        "    return scores\n",
        "\n",
        "def evaluate_criterion(response, criterion):\n",
        "    # 기준별 평가를 위한 자리표시자\n",
        "    # 실제로는 NLP 기법, 루브릭 기반 평가 또는 다른 LLM을 포함할 수 있다\n",
        "    return 0  # 자리표시자 반환\n",
        "\n",
        "# 사용 예\n",
        "criteria = [\"Accuracy\", \"Clarity\", \"Completeness\", \"Conciseness\"]\n",
        "\n",
        "evaluation = evaluate_Reflection_impact(initial_response, corrected_response, criteria)\n",
        "print(\"평가 결과:\")\n",
        "print(f\"초기 점수: {evaluation['initial_scores']}\")\n",
        "print(f\"반성 점수: {evaluation['Reflection_scores']}\")\n",
        "print(f\"영향: {evaluation['impact']}\")\n"
      ],
      "metadata": {
        "id": "RzyP7z8lV94m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 효과적인 반성 구현의 과제"
      ],
      "metadata": {
        "id": "I0a3IDf0WCEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def controlled_Reflection(\n",
        "    model, tokenizer, task, max_iterations=3, improvement_threshold=0.1\n",
        "):\n",
        "    response = generate_initial_response(model, tokenizer, task)\n",
        "    previous_score = evaluate_response(response, [\"Overall_Quality\"])[\"Overall_Quality\"]\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        improved_response = apply_Reflection(model, tokenizer, task, response)\n",
        "        current_score = evaluate_response(\n",
        "            improved_response, [\"Overall_Quality\"]\n",
        "        )[\"Overall_Quality\"]\n",
        "\n",
        "        if current_score - previous_score < improvement_threshold:\n",
        "            break\n",
        "\n",
        "        response = improved_response\n",
        "        previous_score = current_score\n",
        "\n",
        "    return response\n",
        "\n",
        "def apply_Reflection(model, tokenizer, task, response):\n",
        "    # 반성의 단일 패스를 구현\n",
        "    pass\n",
        "\n",
        "# 사용 예시\n",
        "task = \"상대성 이론을 설명하라.\"\n",
        "final_response = controlled_Reflection(model, tokenizer, task)\n",
        "print(final_response)\n"
      ],
      "metadata": {
        "id": "XkIfQad0WF2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 미래 방향"
      ],
      "metadata": {
        "id": "s0yJ5GEnWH8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_agent_Reflection(\n",
        "    models, tokenizers, task, num_agents=3\n",
        "):\n",
        "    responses = [\n",
        "        generate_initial_response(\n",
        "            models[i], tokenizers[i], task\n",
        "        )\n",
        "        for i in range(num_agents)\n",
        "    ]\n",
        "\n",
        "    for _ in range(3):  # 반성 횟수\n",
        "        Reflections = []\n",
        "        for i in range(num_agents):\n",
        "            other_responses = responses[:i] + responses[i+1:]\n",
        "            reflection = generate_Reflection(\n",
        "                models[i], tokenizers[i], task,\n",
        "                responses[i], other_responses\n",
        "            )\n",
        "            Reflections.append(Reflection)\n",
        "\n",
        "        responses = [extract_improved_response(Reflection)\n",
        "            for reflection in Reflections]\n",
        "    return select_best_response(responses)\n"
      ],
      "metadata": {
        "id": "C1eLmyFbWKmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_Reflection(\n",
        "    model, tokenizer, task, own_response, other_responses\n",
        "):\n",
        "    prompt = f\"\"\"과업: {task}\n",
        "\n",
        "당신의 응답:\n",
        "{own_response}\n",
        "\n",
        "다른 응답들:\n",
        "{' '.join(f'- {response}' for response in other_responses)}\n",
        "\n",
        "다른 응답들을 고려해 자신의 응답을 반성하라. 각 접근 방식의 장점과 단점을 식별하고 모든 관점에서 최고의 요소들을 통합한 개선된 응답을 제안하라.\n",
        "\n",
        "당신의 반성과 개선된 응답:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        inputs, max_length=1500, num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def select_best_response(responses):\n",
        "    # 여러 응답들에서 최고의 요소들을 선택하거나 결합하는 논리를 구현\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "bhy_RoiQWN0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"도시 교통 혼잡을 줄이기 위한 솔루션을 제안하라.\"\n",
        "final_response = multi_agent_Reflection(models, tokenizers, task)\n",
        "print(final_response)\n"
      ],
      "metadata": {
        "id": "BV9qLyuBWQ5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}